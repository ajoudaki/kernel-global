\documentclass[twoside]{article}

\usepackage{aistats2025}

% \usepackage{biblatex}
\usepackage{natbib}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}

\newcommand{\avg}{\text{avg}}
\newcommand{\res}{\text{res}}
\newcommand{\E}{\mathbb{E}\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\he}{\mathrm{he}}
\newcommand{\He}{\mathrm{He}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
% \newtheorem{corollary}[theorem]{Corollary}
\newcommand{\thomas}[1]{{\color{blue}TH:  \textit{#1}}}
\newcommand{\amir}[1]{{\color{cyan}AJ:  \textit{#1}}}
\graphicspath{ {./figures/} }


\begin{document}

% Title and authors
\twocolumn[
\aistatstitle{Emergence of Globally Attracting Fixed Points in\\ Deep Neural Networks With Nonlinear Activations}
\aistatsauthor{Anonymous Author(s)}
\aistatsaddress{Affiliation(s)} ]

\begin{abstract}
Understanding how neural networks transform input data across layers is fundamental to unraveling their learning and generalization capabilities. Although prior work has used insights from kernel methods to study neural networks, a global analysis of how the similarity between hidden representations evolves across layers remains underexplored. In this paper, we introduce a theoretical framework for the evolution of the kernel sequence, which measures the similarity between the hidden representation for two different inputs. Operating under the mean field regime, we show that the kernel sequence evolves deterministically via a kernel map, which only depends on the activation function. By expanding activation using Hermite polynomials and using their algebraic properties, we derive an explicit form for kernel map and fully characterize its fixed points. Our analysis reveals that for nonlinear activations, the kernel sequence converges globally to a unique fixed point, which can correspond to orthogonal or similar representations depending on the activation and network architecture. We further extend our results to networks with normalization layers, demonstrating similar convergence behaviors. This work provides new insights into the implicit biases of deep neural networks and how architectural choices influence the evolution of representations across layers.
\end{abstract}


% \section{Fixed points and global convergence of deep representations}
% \label{ch:isometry_activation}
\section{Introduction}
Deep neural networks have revolutionized various fields, from computer vision to natural language processing, due to their remarkable ability to learn complex patterns from data. Understanding the internal mechanisms that govern their learning and generalization capabilities remains a fundamental challenge. 

One approach to studying these transformations is through the lens of kernel methods. Kernel methods have a long history in machine learning for analyzing relationships between data points in high-dimensional spaces \citep{scholkopf2002learning, smola2004tutorial}. They provide a framework for understanding the similarity measures that underpin many learning algorithms. Recent theoretical studies have increasingly focused on analyzing neural networks from the perspective of kernels. The Neural Tangent Kernel (NTK) introduced by \citet{jacot2018neural} is a seminal work that provided a way to analyze the training dynamics of infinitely wide neural networks using kernel methods. This perspective has been further explored in various contexts, leading to significant advances in our understanding of neural networks \citep{lee2019wide, arora2019exact, yang2019scaling}.

Despite these advances, an important question remains unexplored: \emph{How does the similarity between hidden layer representations evolve across layers, and how is that affected by particular choices of nonlinear functions?} Previous work has mainly focused on local behaviors or specific initialization conditions \citep{saxe2013exact, schoenholz2016deep, pennington2017resurrecting}. A comprehensive global analysis of fixed points and convergence properties of kernel sequences, particularly in the presence of nonlinear activations, is still incomplete.

This paper addresses this gap by introducing and analyzing the evolution of kernel sequences in deep neural networks. Specifically, we consider the kernel sequence $k(h^\ell(x), h^\ell(y))$, where $k$ denotes a similarity measure, and $h^\ell(x)$ and $h^\ell(y)$ are representations of the inputs $x$ and $y$ at layer $\ell$. Understanding whether and how this sequence converges to a fixed point as the depth of the network increases, is crucial for uncovering the inherent implicit biases of deep networks.

Our analysis builds upon foundational work in neural network theory, and leverages mean field theory to simplify the analysis. By considering the infinite-width limit, stochastic sequences become deterministic, allowing us to focus on the underlying dynamics without the interference of random fluctuations \citep{poole2016exponential, yang2019meanfield, mei2019mean}.

\textbf{Contributions:} 
\begin{itemize}
    \item By employing algebraic properties Hermite polynomials, we derive explicit forms of the neural kernel and identify its fixed points, leading to many elegant results.  
    \item We demonstrate that for a wide class of activation functions, the kernel sequence converges globally to a unique fixed point, revealing inherent biases in deep representations. 
    \item We extend our analysis to networks with normalization layers and residual connections, highlighting their impact on the convergence behavior.
\end{itemize}

Understanding these dynamics contributes to a deeper understanding of how depth and nonlinearity interact with one another at initialization, shedding light on their representational capacities.


\section{Related works}
The study of deep neural networks through the lens of kernel methods and mean field theory has garnered significant interest in recent years. The Neural Tangent Kernel (NTK) introduced by \citet{jacot2018neural} provided a framework to analyze the training dynamics of infinitely wide neural networks using kernel methods. This perspective was further expanded by \citet{lee2019wide} and \citet{arora2019exact}, who explored the connections between neural networks and Gaussian processes.

The propagation of signals in deep networks has been studied using mean field theory, as seen in the works of \citet{schoenholz2016deep} and \citet{pennington2017resurrecting}.  Previous studies have also explored the critical role of activation functions in maintaining signal propagation and stable gradient behavior in deep networks \citep{hayou2019impact}. These studies focused on understanding the conditions required for the stable propagation of information and the avoidance of signal amplification or attenuation. However, these analyses often concentrated on local behaviors or specific conditions, leaving a gap in understanding the global evolution of representations across layers. 

Hermite polynomials have been used in probability theory and statistics, particularly in the context of Gaussian processes \citep{williams2006gaussian}. Although \citet{poole2016exponential} and \citet{daniely2016toward} have utilized polynomial expansions to analyze neural networks, they do not study global dynamics in neural networks, as presented in this paper. To the best of our knowledge, the only existing work that uses Hermite polynomials in the mean field regime to study global dynamics of the kernel is \citet{joudaki2023impact}. However, this study only covers centered activations, which fail to capture several striking global dynamics covered in this study.

Our work extends these foundational studies by providing an explicit algebraic framework to analyze the global convergence of kernel sequences in deep networks with nonlinear activations. Using Hermite polynomial expansions, we offer a precise characterization of the kernel map and its fixed points, contributing new insights into the implicit biases and representational dynamics of deep neural networks.

\section{Preliminaries}
In this section, we introduce the fundamental concepts, notations, and definitions used throughout this paper. 
If $X$ and $Y$ are vectors in $\mathbb{R}^n$, we use the inner product notation $\langle X, Y \rangle_\avg$ to denote their average inner product $ = \frac{1}{n} \sum_{i=1}^n X_i Y_i.$  We consider a feedforward neural network with $L$ layers and constant layer width $d$. The network takes an input vector $x \in \mathbb{R}^d$ and maps it to an output vector $h^L(x) \in \mathbb{R}^d$ through a series of transformations. The hidden representations in each layer $\ell$ are denoted by $h^\ell(x)$. The transformation in each layer is composed of a linear transformation followed by a nonlinear activation function $\phi$. We consider the multilayer perceptron (MLP), the hidden representation at layer $\ell$ is given by:
\begin{align}
& h^\ell(x) = \phi\left(\frac{1}{\sqrt{d}}W^\ell h^{\ell-1}(x)\right), && W^\ell \in \mathbb{R}^{d \times d},
\end{align}
where $h^0(x)=x$ is identified with the input. We can assume that elements of $W^\ell$ are assumed to be drawn i.i.d.~from a zero mean distribution, unit variance distribution. Two prominent choices for weight distributions, Gaussian $N(0,1)$ and uniform distribution $\text{Uniform}[-1,1],$ satisfy these conditions. In some variations of the MLP, we will use normalization layers to adjust the activations at each layer, namely Layer Normalization (LN) or Root Mean Squared (RMS) normalization. Finally, the neural kernel between two inputs $x$ and $y$ at layer $\ell$ is defined as:
\begin{align}
    \rho_\ell = \langle h^\ell(x), h^\ell(y) \rangle_\avg \,.
\end{align}

The main goal of this paper is to analyze the sequence $\{\rho_\ell\}$ at random initialization. The motivation for this analysis is to understand if architectural choices, specifically the activation function and the model depth, lead to specific biases of the kernel towards a certain fixed point. Namely, if there is a bias towards zero, it would imply that at initialization, the representations become more orthogonal. In contrast, a positive fixed point would mean that the representations become more similar.

In the current setup, the sequence $\{\rho_\ell\}$ is a stochastic sequence or a Markov chain due to the random weights at each layer. However, we will show that under the mean field regime, the sequence $\{\rho_\ell\}$ converges to a deterministic sequence, and we will analyze its properties.


\section{Mean field Regime}

In this section, we conduct a mean field analysis of MLP to explore the neural kernel's fixed point behavior as the network depth increases. This approach allows us to gain insight into the global dynamics of neural networks, mainly how the similarity between two input samples evolves as they pass through successive network layers.
Now, we can state the mean field regime for the kernel sequence, stating that in this regime, the sequence becomes deterministic. 

As will be proven later, the deterministic transition of this sequence follows a scalar function that is defined below. 

\begin{definition}
    \label{def:kernel_map}
Given two random variables $X, Y$ with covariance $\rho,$ and activation $\phi,$ define the \emph{kernel map } $\kappa$ as the mapping between the covariance of preactivations and the covariance of post-activations:
\begin{align}
& \kappa(\rho):=\E\phi(X)\phi(Y), && 
 \begin{pmatrix}X \\ Y\end{pmatrix}\sim \mathcal N\left(0, \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}
 \right)
 \label{eq:kernel_map}
\end{align}
\end{definition}

The following proposition formally states that in the mean field regime, the kernel sequence follows iterative applications of kernel map. 

\begin{proposition}
\label{prop:mean_field_kernel_general}
In the mean field regime with $d \to \infty$, let $\rho_\ell$ denote the kernel sequence of an MLP with activation function $\phi$ obeying $\E\phi(X)^2=1$ for $X\sim \mathcal N(0,1)$. If each element of the weights is drawn i.i.d.~from a distribution with zero mean and unit variance, the kernel sequence evolves deterministically as follows:
\begin{align*}
&\rho_{\ell+1} = \kappa(\rho_\ell),
\end{align*}
where the initial value $\rho_0$ corresponds to the input, and $\kappa$ is defined in Definition~\ref{def:kernel_map}. 


\end{proposition}

Historically, conditions similar to $\E \phi(X)^2=1$ have been used to prevent quantities on the forward pass from vanishing or exploding. For example, in a ReLU half of the activations will be zeroed out, which will lead to a vanishing norm of forward representations. The initialization of
\citep{he2016deep} addresses that by scaling weights to maintain consistent forward pass norms across layers. This principle is further refined by \citet{klambauer2017self}, proposing the idea of self-normalizing activation functions, which ensure consistent mean and variances between pre- and post-activations. 

An interesting observation is that kernel dynamics is governed by kernel map $\kappa,$ defined on the basis of Gaussian preactivations, even if the weights are not Gaussian matrices. The main insight for this equivalence is that as long as elements are drawn i.i.d.~from a zero mean unit variance distribution, we can apply the Central Limit Theorem (CLT) to conclude that preactivations follow the Gaussian distribution. This simple yet elegant observation gives us a very powerful tool to study kernel dynamics by leveraging algebraic properties for Gaussian preactivations, and automatically gets the same results for a wide class of distributions. 

The proposition~\ref{prop:mean_field_kernel_general} tells us that the sequence is given by $\rho,\kappa(\rho),\kappa(\kappa(\rho)),\dots\,.$ Thus, we can analyze its convergence by studying the fixed points of the kernel map $\kappa,$
which are the values of $\rho^*$ that satisfy $\kappa(\rho^*) = \rho^*.$  With the assumption that $\E \phi(X)^2=1,$ the kernel map $\kappa$ is a mapping between $[-1,-1]$ to itself. Thus, Brower's fixed point theorem implies that the kernel map $\kappa$ has at least one fixed point $\rho^*.$ However, as we will show, there is potentially more than one fixed point, and it will be interesting to understand which ones are locally or globally attractive. 


\section{Hermite expansion of activation functions}
Hermite polynomials possess completeness and orthogonality under the Gaussian measure. Therefore, any function that is square-integrable with respect to a Gaussian measure can be expressed as a linear combination of Hermite polynomials (see below). The square integrability of rules out possibility of having heavy-tailed post-activations without second moments. This holds for all activations that are used in practice. We use \emph{normalized} Hermite polynomials and their coefficients. 

\begin{definition}
Normalized Hermite polynomials $\he_k(x)$ are defined as follows
\begin{align*}
&\he_k(x) :=\frac{1}{\sqrt{k!}}(-1)^k e^{-\frac{x^2}{2}} \frac{d^k}{dx^k} e^{-\frac{x^2}{2}}.
\end{align*}
\end{definition}

Although Hermite polynomials have been used in probability theory and statistics, particularly in the context of Gaussian processes \citep{williams2006gaussian}, their application in analyzing neural network dynamics provides a novel methodological tool. Previous works, such as \citet{daniely2016toward}, have utilized polynomial expansions to study neural networks, but our explicit use of Hermite polynomial expansions to derive the kernel map and analyze convergence is a new contribution.


The crucial property of Hermite polynomials is their  orthogonality:
\begin{align}\label{eq:hermite_orthogonality}
\E\he_k(X)\he_l(X) = \delta_{kl}, \quad X \sim \mathcal N(0,1).
\end{align}
Scaling by $1/\sqrt{k!}$ ensures that polynomials form a \emph{orthonormal} basis.  Based on this property, we can define:
\begin{definition}\label{def:hermite_expansion}
Given a function $\phi$, square-integrable with respect to the Gaussian measure. Its Hermite expansion is given by:
\begin{align*}
\phi = \sum_{k=0}^\infty c_k \he_k,\quad c_k = \E\phi(X) \he_k(X), \; X \sim \mathcal N (0,1)\,.
\end{align*}
Here, $c_k$ are called the Hermite coefficients of $\phi$. 
\end{definition}

% Based on the orthogonality of Hermite polynomials, we can make a few observations about some of the Hermite coefficients.  

In addition to orthogonality, Hermite polynomials have another ``magical'' property that is crucial for our later analysis. 

\begin{lemma}[Mehler's lemma]\label{lem:mehler_kernel}
For standard-normal random variables $X,Y$ with covariance $\rho$ it holds
\begin{align*}
\E\he_m(X)\he_n(Y) = \rho^n \delta_{mn}, && \begin{pmatrix}
    X \\ Y
\end{pmatrix}\sim \mathcal N\left(0, \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix}
 \right).
\end{align*}
\end{lemma}

This lemma states that given two Gaussian random variables $X, Y$ with covariance $\rho$, the expectation of the product of Hermite polynomials is zero unless the indices are equal (see Section~\ref{sec:proofs} for proof).
The lemma~\ref{lem:mehler_kernel} is crucial for our theory. Based on this lemma, we can express the kernel map $\kappa$ in terms of the Hermite coefficients, showing a particular structure of the kernel map.

\begin{corollary}
    \label{cor:hermite_covariance}
    \label{cor:kernel_map}
We have the following explicit form for kernel map:
\begin{align*}
\kappa(\rho) = \sum_{k=0}^\infty c_k^2 \rho^k.
\end{align*}
\end{corollary}


The proof follows directly from the definition of kernel map, expanding $\phi$ in the Hermite basis and then applying Lemma~\ref{lem:mehler_kernel}. 

Although the structure of kernel map as shown in this Corollary have been observed before (cf. ~\citet{daniely2016toward}), to the best of our knowledge, no study has used these algebraic properties to study the global kernel dynamics. 

% Before we turn our attention to convergence, let us draw some links between the properties of activation functions, its kernel map, and its Hermite coefficients.
% \thomas{I am not sure I understand the table below. Does the first case include $\tanh$?}

% \begin{table}[ht]
% \small 
%     \centering
%     \caption{Properties of activations in terms of Hermite coefficients and kernel map: 1) centered, 2) stable, 3) nonlinearity. }
%     \begin{tabular}{|c|c|c|c|}
%     \hline 
%  Prop & Activation $\phi$ & Coefs. $\{c_k\}_{k\ge 0}$ & Kernel map $\kappa$ \\
%     \hline
%  1 & $\E \phi(X)=0$ & $c_0 = 0$ & $\kappa(0) = 0$ \\
%     \hline
%  2 & $\E \phi(X)^2 = 1$ & $\sum_{k=0}^\infty c_k^2 = 1$ & $\kappa(1) = 1$ \\
%     \hline
%  3 & $\phi(x)$ nonlinear & $\sum_{k=2}^\infty c_k^2 > 0$ & $\kappa(\rho)$ nonlinear \\
%     \hline
%     \end{tabular}
% \end{table}

% \section{Global attraction towards orthogonality for centered activations}

% Recall that for activations that obey $\E \phi(X)^2 = 1$, the kernel map  $\kappa$ is a power series with non-negative coefficients $\sum_{k=0}^\infty c_k^2 \rho^k$ that maps $[-1,1]$ to itself, and obeys $\kappa(1)=1.$ This immediately reveals that $\rho=1$ is a fixed point of the kernel map. 
% % This is unsurprising, as it reaffirms the fact that if two inputs are identical with unit variance, the post-activations will also be identical. \thomas{I find the term "identical" misleading. Same distribution?}  
% However, it is far more interesting whether this fixed point is globally attractive and how the kernel map influences the convergence rate to this fixed point. In a similar vein, we can ask if there are other fixed points and how the kernel map influences the convergence rate to these fixed points.





\section{Convergence of the kernel with general activations}
So far, we only discussed the convergence of the kernel map for activations that are centered and the convergence of their kernel sequence towards zero. However, we can extend this analysis to general activations that are not centered. In this section, we will show that for any non-linear activation function, there is a unique fixed point $\rho^*$ that is globally attractive.


\begin{theorem}
\label{thm:global_attract}
Given a nonlinear activation function $\phi$ with kernel  $\kappa$ such that $\kappa(1)=1$. Define the kernel sequence $\rho_{\ell+1}=\kappa(\rho_\ell)$, with $\rho_0 \in (-1,1)$. Then there is a unique globally contracting fixed point $\rho^*$, which is necessarily non-negative $\rho^*\in[0,1]$. The only other fixed points distinct from $\rho^*,$ could be $\pm 1,$ neither of which is stable. Furthermore, we have the following contraction rate towards $\rho^*$:
\begin{enumerate}
    \item If $\kappa(0)=0$ then $\rho^*=0$ is an attracting with rate 
    \begin{align*}
    \frac{|\rho_\ell|}{1-|\rho_\ell|} \le \frac{|\rho_0|}{1-|\rho_0|} \alpha^\ell, && \alpha:=\frac{1}{2-\kappa'(0)}.
    \end{align*}
    % \thomas{You introduced $\Phi$ above. Maybe one should use it here or not use the shortcut at all.}
    \item If $\kappa(0)>0$ and $\kappa'(1)<1$, then $\rho^*=1$ is attracting, with rate 
    \begin{align*}
    |\rho_\ell-1| \le |\rho_0-1| \alpha ^\ell, && \alpha := \kappa'(1).
    \end{align*}
    \item If $\kappa(0) > 0$, and $\kappa'(1)=1$ then  $\rho^*=1$ is attracting with rate 
    \begin{align*}
    |\rho_\ell-1| \le \frac{|\rho_0-1|}{\ell\alpha|\rho_0-1|+1}, && \alpha = 1-\kappa(0)-\kappa'(0).
    \end{align*}
    \item If $\kappa(0) > 0$, and $\kappa'(1)>1$ then the attracting fixed point is necessarily in the range $\rho^*\in(0,1)$, satisfying $\kappa'(\rho^*) < 1,$ for which we have  
    \begin{align*}
    &|\rho_\ell-\rho^*| \le \frac{|\rho_0-\rho^*|}{1-|\rho_0|}\alpha^\ell \\ &\alpha = \max\left\{1-\kappa(0),\kappa'(\rho^*),\frac{1-\rho^*}{2-\kappa'(\rho^*)}\right\},
    \end{align*}
    where $\alpha<1$.
    % \thomas{I have dropped the `if is nonlinear' and simply restricted to nonlinear activations from the start. Hope that is ok.}
\end{enumerate}
\end{theorem}

\paragraph{Implications.}  Let us take a step back and review the main take way of Theorem~\ref{thm:global_attract}. Omitting the constants and for sufficiently large depth $\ell,$ we have 
\begin{align*}
    \langle h^{\ell}(x), h^{\ell}(y)\rangle_\avg =  \begin{cases}
          0+ O(\alpha ^ \ell \langle x, y \rangle_\avg)  & \text{case 1}\\
         1 + O(\alpha ^ \ell \langle x, y \rangle_\avg)  & \text{case 2}\\
         1 + O \left(\ell^{-1}\langle x, y \rangle_\avg \right)   & \text{case 3}\\
         \rho^* + O(\alpha ^ \ell \langle x, y \rangle_\avg)  & \text{case 4},\\
    \end{cases}
\end{align*}
where $\langle x,y\rangle_\avg $ denotes the input similarly. 

Broadly speaking, we can think of three categories of bias:

\begin{itemize}
    \item \textit{Orthogonality bias $\rho^*=0$}: Implies that as the network becomes deeper, representations are pushed towards orthogonality, exponentially fast. We can think of this case as bias towards independence. 
    \item \textit{Weak similarity bias $\rho^*\in (0,1)$}: Implies that as the network becomes deeper, the representations form angles between $0$ and $\pi/2.$ Thus, in this case, the representations are neither completely aligned nor completely independent. 
    \item \textit{Strong similarity $\rho^*=1$}: Implies that as the network becomes deeper, representations become more similar or aligned, as indicated by inner products converging to one, exponentially fast in case 2, polynomially in case 3. 
\end{itemize}

The bias of activation and normalization layers has been extensively studied in the literature. For example, \citet{daneshmand2021batch} show that batch normalization with linear activations makes representations more orthogonal, relying on a technical assumption that is left unproven (see assumption $\mathcal{A}_1$). Similarly, \citet{joudaki2023bridging} extend this to odd activations, yet introduce another technical assumption about ergodicity of the chain, which is hard to verify or prove (see Assumption 1). \citet{yang2019meanfield} prove global convergence of Gram matrix of a network with linear activation and batch normalization (see Corollary F.3) toward a fixed point. However,  authors explain that because they cannot establish such a global convergence (see page 20, under the paragraph titled Main Technical Results), they resort to finding locally stable fixed points, meaning that if preactivations have such a Gram matrix structure and they are perturbed infinitesimally.  Recent studies have also examined the evolution of covariance structures in deep and wide networks using stochastic differential equations \citep{li2022neural}, which is similar to kernel dynamics and kernel ODE introduced here. However, the covariance SDE approach does not theoretically show the global convergence towards these solutions. Finally, \citet{joudaki2023impact} establish a global bias towards orthogonality for centered activations (see Theorem A2), which aligns with case 1 of Theorem~\ref{thm:global_attract}, and to extend to other activations, they add layer normalization after the activation to make it centered. In contrast, Theorem~\ref{thm:global_attract} covers all existing activations. 

One alternative way of interpreting the results of Theorem~\ref{thm:global_attract} is that as we pass two inputs through nonlinear activations, it `forgets' the similarity between inputs, and converges to a fixed point value that is independent of the inputs and only depends on the activation. Furthermore, the more nonlinear the activation, the faster this convergence. If we take the view that the network ought to remember some similarity of the input to be able to start training, we can leverage Theorem~\ref{thm:global_attract} to strike a balance between depth and nonlinearity of the activations. 


One of the most striking results of this theorem is that for any nonlinear activation, there is exactly one fixed point $\rho^*$ that is globally attractive, while other fixed points are not stable. Furthermore, the fixed point is necessarily non-negative. This implies that for any MLP with nonlinear activations, the covariance of preactivations will converge towards a fixed point, which is always non-negative. It turns out that there is a geometric reason why no negative fixed points could exist (See remark~\ref{rem:no_negative_geometric})


% Note that in some of the cases of Theorem~\ref{thm:global_attract}, we showed contraction under the potential $|\rho|/(1-|\rho|)$ Because $|\rho|/(1-|\rho|)$ is a monotonically increasing with $|\rho|$, the contraction of $|\rho_\ell|/(1-|\rho_\ell|)$ implies contraction of $|\rho_\ell|$ towards zero. 
There are several aspects of Theorem~\ref{thm:global_attract} that may seem counterintuitive. For example, in case 1, it is shown that a quantity $|\rho|/(1-|\rho|)$ decays exponentially in depth. To shed more light on the underlying theories behind Theorem~\ref{thm:global_attract} 
While this form might seem mysterious, there is an elegant and intuitive way of understanding it by casting our layer-wise kernel dynamics with a continuous-time dynamical system, which is discussed next. For a detailed proof of Theorem~\ref{thm:global_attract} see Section~\ref{sec:proofs}.

\subsection{A continuous time differential equation view to the kernel dynamics} 
One of the most helpful ways to find insights into discrete fixed point iterations is to cast them as a continuous problem. More specifically, consider our fixed point iteration: 
\begin{align*}
    \Delta \rho_\ell &= \rho_{\ell+1} - \rho_\ell = - \rho_\ell + \sum_{k=0}^\infty c_k^2 \rho^k_\ell.
\end{align*}
We can replace this discrete iterations by a continuous time differential equation, which we will refer to as the kernel ODE:
\begin{align}\tag{kernel ODE}\label{eq:kernel_ODE}
    d\rho/dt = -\rho + \sum_{k=0}^\infty c_k^2 \rho^k.
\end{align}
In most scenarios, kernel ODE will capture most important parts of the discrete kernel dynamics.  The continuous analogue of fixed points is a $\rho^*$ that satisfies $\rho'(\rho^*) = 0.$ Furthermore, the fixed point is globally attracting if we have $\rho'$ become negative for $\rho>\rho^*$ and positive for $\rho<\rho^*.$ We can say that the fixed point is locally attracting if this property only holds in some small neighborhood of $\rho^*.$

While the kernel ODE presents an interesting transformation of the problem, it does not have a closed form solution in its general form. However, our main strategy is to find worst case (slowest) scenarios for convergence that correspond to tractable ODEs. In many cases however, the worst case scenario depends on some boundary conditions. Thus, we can design worst case ODE for different cases and combine them in a joint bound. Let us use our centered activation equation as a case study. 

\subsection{Kernel ODE for centered activations}
For the case of centered activation $\E \phi(X)=0,$ corresponding to Case 1 of Theorem~\ref{thm:global_attract}, the kernel ODE is given by
\begin{align*}
    d\rho/dt = -(1-c_1^2)\rho + \sum_{k=2}^\infty c_k^2\rho^k\,,
\end{align*}
where the first term $k=0$ is canceled due to the assumption $\kappa(0)=c_0^2 = 1.$ 

Observe that $\rho'<0$ when $\rho>0,$ and $\rho'>0$ when $\rho<0.$ This implies that $\rho(t) \to 0$ for sufficiently large $t.$ Intuitively, the terms in $\rho'$ that have the opposite sign of $\rho,$ contribute to a faster convergence, while terms with a similar sign contribute to a slower convergence. Thus, we can ask what distribution of Hermite coefficients $\{c_k\}$ corresponds to the worst case, slowest convergence. If this is a tractable ODE, we can use its solution to bound the convergence of the original ODE. It turns out that the worst case depends on the positivity of $\rho,$ which is why we study them separately.

\textit{Positive range.}
Let us first consider the dynamics when $\rho\ge 0.$ 
In this case term corresponding to $k=1$ contribute positively to a faster convergence, while terms $k\ge 2$ make convergence slower. In light of this observation, a worst case (slowest possible) convergence rate happens when the positive terms are maximized, which occurs when the weight of all $\sum_{k=2}^\infty c_k^2 $ is concentrated on the $k=2$ term, leading to kernel ODE
% \thomas{That makes sense. What is the worst case $\phi$? Does it make sense to state this for better illustration?}
\begin{align*}
    d\rho/dt &=  -(1-c_1^2)\rho + (1-c_1^2) \rho^2,
    % \\
    % &= -(1-c_1^2) \rho(1-\rho).
\end{align*}
where we used the fact that $\sum_{k=1}^\infty c_k^2 = 1.$ 
Finally, we can solve this ODE 
\begin{align*}
    &\int \frac{d\rho}{\rho(1-\rho)} = - (1-c_1^2)\int t \\
    % \implies \ln \frac{|\rho(t)|}{|1-\rho(t)|} = -t (1-c_1^2) + C\\
    \implies &\frac{|\rho(t)|}{|1-\rho(t)|} = C \exp(-t(1-c_1^2)),
\end{align*}
where $C$ corresponds to the initial values. 
% Solving for the constant and assuming that our continuous approximation of the discrete equation is accurate, we can state that approximately:
% \begin{align*}
% \ln \frac{|\rho_\ell|}{|1-\rho_\ell|} =  \ln \frac{|\rho_0|}{|1-\rho_0|} -(1-\kappa'(0))\ell,
% \end{align*}
% where we removed absolute values around $1-\rho_\ell$ and $1-\rho_0$ because $\rho_\ell\le 1$ for all $\ell.$
% Observe that this is largely equivalent to the exact bound found in Theorem~\ref{thm:global_attract_centered}. 
% % \thomas{Is it safe to drop the absolute values around the $\rho_\ell$'s? You had them before.} 


\textit{Negative range.}
Now let us assume $\rho<0.$ In this case, only the odd terms in $\sum_{k=2}^\infty c_k^2 \rho^k$ contribute to a slowdown in convergence. Thus, the worst case occurs when all the weight of coefficients is concentrated in $k=3,$ leading to the kernel ODE:
\begin{align*}
&d\rho/dt = -(1-c_1^2)\rho + (1-c_1^2) \rho^3\\
% \implies    &\int \frac{d\rho}{\rho(1-\rho^2)} = -(1-c_1^2)\int dt \\
% \implies \ln\left(\frac{|\rho(t)|}{\sqrt{1-\rho^2(t)}}\right) = - t (1-c_1^2) + B\\
\implies &\frac{|\rho(t)|}{\sqrt{1-\rho^2(t)}} = C' \exp(-t(1-c_1^2)),
\end{align*}
where $C'$ corresponds to the initial values. 

To summarize, we have obtained:
\begin{align*}
    \begin{cases}
        \frac{|\rho(t)|}{1-\rho(t)} \le C \exp(-\ell(1-c_1^2)) & \rho_0 \in \R^+\\
        \frac{|\rho(t)|}{\sqrt{1-\rho(t)^2}}\le C' \exp(-\ell(1-c_1^2)) & \rho_0\in\R^-.
    \end{cases}
\end{align*}
Now, we can use a numerical inequality $\sqrt{1-x^2} \ge 1-|x|$ valid for all $x\in(-1,1),$ and by solving for the constant, we can construct a joint bound for both cases:
\begin{align*}
    \frac{|\rho(t)|}{1-|\rho(t)|} \le \frac{|\rho_0|}{1-|\rho_0|} \exp(-t (1-\kappa'(0))), 
\end{align*}
where we have also replaced $c_1^2=\kappa'(1).$  

\paragraph{Key insights.}
The kernel ODE perspective reveals two important insights. 
First, the appearance of $|\rho|/(1-|\rho|)$ formula in our bound is due to the fact that $x/(1-x) = \exp( -c t) $ is a fundamental solution of the differential equation $x' = - c x(1-x).$ Second, it reveals the importance of positivity of coefficients in $\sum_{k=0}^\infty c_k^2\rho^k,$ which allowed us to arrive at a worst case pattern for the coefficients. This further highlights the value of algebraic properties of Hermite polynomials that canceled out the cross terms in the kernel map. 

We can observe that the kernel ODE rate derived by analysis of kernel ODE is largely aligned exponential convergence of this term in Theorem~\ref{thm:global_attract_centered}. The slight discrepancy between the exponential rates between Theorem~\ref{thm:global_attract_centered} and the kernel ODE rate $\exp(-(1-\kappa'(0))$ is due to the discrete-continuous approximation. Despite this small discrepancy, the solution to kernel ODE can help us arrive at the right form for the solution. We can leverage the solution to kernel ODE to arrive at a bound for the discrete problem, namely using induction over steps.

It might be worth that the slowest convergence in the positive range when $\phi  = c_1 \he_1 + \sqrt{1-c_1^2}\he_2,$ and in the negative range when $\phi  = c_1 \he_1 + \sqrt{1-c_1^2}\he_3.$ Since Theorem~\ref{thm:global_attract} only provides an upper bound and not lower bound for convergence, one natural question is how big is the gap between this worst case and the exact ODE convergence rates? It turns out that we can construct activations were the convergence is substantially faster, such as doubly exponential for certain functions (See Corollary~\ref{cor:double_exp}). However, for all practically used activations the gap our bound and the real convergence is enough (See section~\ref{sec:experiments}).



% The main proof idea of this theorem is to show that upon applying the kernel map $\kappa$ to the covariance of preactivations, the potential function $|\rho|/(1-|\rho|)$ decreases with rate $\alpha,$ and applying an induction over $\ell.$ Condition $\kappa(1)=\E \phi(X)^2 = 1$ ensures that activations are stable and do not explode or vanish, allowing us to apply the single step contraction inductively. The proof of the single step follows from the properties of Hermite polynomials and the orthogonality of the kernel map. The nonlinearity condition is essential to have $\alpha<1,$ which rules out identity-like activations that do not change the value. The centered condition $\kappa(0)=\E \phi(X) = 0$ ensures that $\rho^*$ is a fixed point.

% Similar to before, we can use the fact that $\ln(x/\sqrt{1-x})=c dt$ is a fundamental solution to equation $x' = c x(1-x^2), $ to approximate the kernel dynamics:
% \begin{align*}
%     \ln\left(\frac{|\rho_\ell |}{\sqrt{1-\rho^2_\ell }}\right) - \ln\left(\frac{|\rho_0 |}{\sqrt{1-\rho^2_0 }}\right) = - t (1-c_1^2) \, .
% \end{align*}
% This corresponds to an activation that can be written as weighted average of first and third Hermite polynomials $\phi(x) = \beta \he_1(x) + (1-\beta) \he_3(x),$ for some $\beta\in(0,1).$ 
% \thomas{I think the conveying this insight is really helpful! Could profit from a little bit more of extra work to increase clarity.}
% 

% Some activations, such as SeLU~\citep{klambauer2017self}, which is a self-normalizing activation function that has $\E \phi(X)^1 = 0.$ 


% \thomas{Yeah, it seems that sign alternations can never reach a fixed point, unless at $0$. Maybe this can just be excluded by a simple argument beforehand. I generally like to prune the space of possibilities early on. At any rate, a discussion of the special case of $\kappa$ taking negative values would probably be valuable.}
 



\section{Extension to normalization layers and residual connections}

In this section, we extend our analysis to MLPs that incorporate normalization layers and residual (skip) connections, and examine how these architectural modifications affect the kernel sequence and its convergence properties.


\subsection{Residual connections}

Residual connections, inspired by ResNets~\citep{he2016deep}, help mitigate the vanishing gradient problem in deep networks by allowing gradients to flow directly through skip connections. We consider the MLP with residual strength $t$ given by
\begin{align*}
h^\ell = r \, \phi\left( \frac{W^\ell}{\sqrt{d}}  h^{\ell-1} \right) + \sqrt{1-r^2}\, \frac{P^\ell}{\sqrt{d}}  h^{\ell-1},
\end{align*}
where $W^\ell$ and $P^\ell$ are independent weight matrices of dimension $d \times d$ with entries drawn i.i.d.~from a zero-mean, unit-variance distribution, and $r \in (0, 1)$ modulates the strength of the residual connections (the smaller $|r|,$ the stronger residuals will be).


\begin{proposition}
\label{prop:residual_kernel_map}
For an MLP with activation $\phi$ satisfying $\E\phi(X)^2,\,X\sim N(0,1),$ and residual parameter $r,$ we have the residual kernel map
\begin{equation}
\kappa_\res(\rho) = r^2 \kappa_\phi(\rho) + (1-r^2) \rho,
\end{equation}
where $\kappa_\phi$ denotes kernel map of $\phi.$

Furthermore, we have: 1) fixed points of $\kappa_\res(\rho)$ are the same as those of $\kappa_\phi(\rho),$ 2)  $\rho^*$ is a globally attracting fixed point of  $\kappa_\phi$ is a globally attracting fixed point of $\kappa_\psi,$ 3) the convergence of residual kernel map $\kappa_\psi$ is monotonically increasing in $r$ (the stronger residuals, the slower convergence). 
\end{proposition}



\paragraph{Proof insight.} First observation is that due to independence of $W^\ell$ and $P^\ell,$ the cross terms in the joint kernel disappear. Thus, the residual connection acts as a linear combination of the transformed input and the input itself, leading to the modified kernel map $\kappa_\res(\rho)$. Furthermore, Because the identity map is neither attractive nor repulsive, the attracting qualities of $\rho^*$ will transfer from $\kappa_\phi$ to $\kappa_\res.$ See section~\ref{sec:proofs} for detailed proof.

\paragraph{Implications.} Proposition~\ref{prop:residual_kernel_map} reveals that residual connections modify the kernel map by blending the original kernel map with the identity function, weighted by the residual $r$. This adjustment has several interesting implications.  Recall that convergence towards a fixed point that indicates that it forgets similarity or dissimilarity between two inputs. While it is not surprising that residual connections help maintain information about inputs, our analysis here gives a quantitative way of how we can balance depth, with nonlinearity of activations, and thereby residual connections.  Another interesting byproduct of proposition~\ref{prop:residual_kernel_map} is that it shows that when $r\to 0$ (highly strong residuals), the kernel ODE dynamics captures the discrete dynamics perfectly (See remark~\ref{rem:residual_ODE}).





% \subsection{Theorem 2: Normalization Layers}

\subsection{Normalization layers}

Normalization layers are widely used in deep learning to stabilize and accelerate training. In this section we focus on two common normalization layers:
\begin{itemize}
    \item \textit{Layer Normalization (LN)} \citep{ba2016layer}:
  \begin{equation}
  \operatorname{LN}(z) = \frac{z - \mu(z)}{\sigma(z)},
  \end{equation}
  where $\mu(z)$ is the mean, and $\sigma(z)$ is the standard deviation of the vector $z \in \mathbb{R}^d$.
  \item \textit{Root Mean Square (RMS) Normalization}:
  \begin{equation}
  \operatorname{RN}(z) = \frac{z}{ \sqrt{ \frac{1}{d} \sum_{i=1}^d z_i^2 } }.
  \end{equation}
\end{itemize}

The following theorem characterizes the joint activation and normalization kernel, depending on the order between two and the type of normalization layer used. 

\begin{proposition}
\label{prop:normalization_kernel_map}
Let us assume $\phi$ satisfies $\E \phi(X)^2 = 1$ where $X\sim N(0,1).$ Consider an MLP layers 
\begin{align*}
    h^\ell = \psi\left( \frac{1}{\sqrt{d}}W^\ell h^{\ell-1}\right),
    \; \psi \in \{\phi\circ RN,\phi\circ LN, RN\circ \phi\},
\end{align*}
where $\psi$ denotes the joint activation and normalization layers. 
We have the following joint normalization and activation kernel:
\begin{align*}
    \kappa_\psi(\rho) = \begin{cases}
         \frac{\kappa_\phi(\rho)-\kappa_\phi(0)}{1-\kappa_\phi(0)} & \text{if } \psi=LN\circ \phi\\
        \kappa_\phi(\rho) & \text{otherwise},\\
    \end{cases}
\end{align*}
where $\kappa_\phi$ denotes the kernel map of $\phi$. 
\end{proposition}

\paragraph{Proof insight.} The main insight from the proof is that in the mean field regime, as the sample means and standard deviation converge to their population counterparts, normalization layers become a rescale and shift with constant values. This manifests itself most clearly when layer normalization is applied after activation, resulting in a modified kernel map that is centered. 
Our extension of kernel dynamics to include Layer Normalization builds upon the foundations laid by\citet{joudaki2023impact} (see Section 5.2 and Theorem 4), but we generalize their results to a broader class of activation functions and network architectures.

\paragraph{Implications.} Proposition~\ref{prop:normalization_kernel_map} indicates that normalization layers adjust the kernel map by scaling it and, in some cases, shifting it. The main takeaway here is that from the kernel point of view, in three out of four configurations, the convergence characteristics of the activations remain the same, but when we apply layer normalization after activation, it makes the activation and kernel map centered $\E \psi(X) = 0,$ and $\kappa_\psi(0)=0$. This implies that for activations such as ReLU or sigmoid, if we apply layer normalization after activation, the fixed point of the kernel sequence changes from $\rho^*=1$ to $=0. $ In other words, it changes from a bias towards orthogonality of representations to aligning them. Our analysis aligns with previous findings by~\citet{joudaki2023impact} for the layer normalization effect in centering, and extends it to other configurations for different orders of layers, as well as using root mean normalization. Our findings also align with studies showing that batch normalization induces orthogonal representations in deep networks \citep{yang2019meanfield,daneshmand2021batch}.


\section{Discussion}
The power of deep neural networks fundamentally arises from their depth and nonlinearity. Despite substantial empirical evidence demonstrating the importance of deep, nonlinear models, a rigorous theoretical understanding of how they operate and learn remains a significant theoretical challenge. Much of classical mathematical and statistical machinery was developed for linear systems or shallow models \citep{hastie2009elements}, and extending these tools to deep, nonlinear architectures poses substantial difficulties. Our work takes a natural first step into this theoretical mystery by providing a rigorous analysis of feedforward networks with nonlinear activations. 

% \paragraph{An algebraic approach.} 
We showed that viewing activations in the Hermite basis uncovers several strikingly elegant properties of activations. Many architectural choices, such as normalization layers and initialization techniques, as well as CLT type convergences, make Gaussian preactivations central to understanding role of activations. Hermite expansion can be viewed as the Fourier analogue for the Gaussian kernel. Thus, viewing activations in the Hermite basis is in many ways more natural than in the raw signal, analogous to viewing convolutions in Fourier basis. For example, the fact that the kernel map $\kappa$ is analytic and has a positive power series expansion, i.e., is infinitely smooth, does not require the activation $\phi$ to be smooth or even continuous. Thus, as opposed to many existing analysis that have to consider smooth and non-smooth cases separately, our theoretical framework gives a unified perspective on them. As an interesting example, smoothness, which has been observed to facilitate better training dynamics\citep{hayou2019impact}, appears as a faster decay in Hermite coefficients.  Thus, similar to leveraging Fourier transform for understanding and designing filters, one can aspire to use Hermite analysis for designing and analyzing activation functions.


% \paragraph{A global perspective. } 
One of the key contributions of our work is the global perspective in analyzing neural networks. Traditional analyses often focus on local structure, such as Jacobian eigenspectrum by \citet{pennington2018emergence}. While these local studies provide many  valuable insights, they do not capture the global kernel dynamics. More concretely, real world inputs to neural networks, such as images of a dog and a cat, are not close enough to each other to be captured in the local changes. Our global approach allows us to study the evolution of representations across layers for any pair of inputs, providing further insights into how deep networks transform inputs with varying degrees of similarity.

% \paragraph{Limitations and future research. } 
One of the central assumptions for our approach is operating in the mean field, i.e., infinite-width regime. While this approach led us to establish numerous elegant properties, it would be worth to explore the differences between finite-width and infinite-width results. This remains an important avenue for future work. Similarly, in the CLT application of Proposition~\ref{prop:mean_field_kernel_general}, the exact discrepancies between the limited width and the infinite width would deepen our understanding of practical neural networks.  Another intriguing endeavor is the potential to extend our framework to develop a theory for self normalizing activation functions, in a vein similar to \citet{klambauer2017self}. Finally, building on the mathematical foundations laid here to study first- and second-order gradients and their impact on training dynamics remains a highly interesting topic for future research. 



In conclusion, our algebraic approach provides a new lens through which to understand the global dynamics of deep neural networks. By leveraging mathematical patterns and symmetries, we have gained profound insights into the evolution of representations and the impact of architectural choices. We hope that this work paves the way for further exploration into the mathematical foundations of deep learning and informs the design of more effective neural network architectures.

\bibliographystyle{plainnat}
\bibliography{refs}


\onecolumn

\section*{Checklist}


% %%% BEGIN INSTRUCTIONS %%%
The checklist follows the references. For each question, choose your answer from the three possible options: Yes, No, Not Applicable.  You are encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description (1-2 sentences). 
Please do not modify the questions.  Note that the Checklist section does not count towards the page limit. Not including the checklist in the first submission won't result in desk rejection, although in such case we will ask you to upload it during the author response period and include it in camera ready (if accepted).

\textbf{In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.}
% %%% END INSTRUCTIONS %%%


 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes]
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Not Applicable]
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes]
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [Yes]
   \item Complete proofs of all theoretical results. [Yes]
   \item Clear explanations of any assumptions. [Yes]     
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes]
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Not Applicable]
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Not Applicable]
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Not Applicable]
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [Not Applicable]
   \item The license information of the assets, if applicable. [Not Applicable]
   \item New assets either in the supplemental material or as a URL, if applicable. [Not Applicable]
   \item Information about consent from data providers/curators. [Not Applicable]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [Not Applicable]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
 \end{enumerate}

 \end{enumerate}

\appendix
\renewcommand{\theremark}{\thesection.\arabic{remark}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\renewcommand{\thetheorem}{\thesection.\arabic{theorem}}
\renewcommand{\thecorollary}{\thesection.\arabic{corollary}}
\renewcommand{\thecorollary}{\thesection.\arabic{lemma}}



\section{Supplementary theorems and proofs}\label{sec:proofs}

\begin{remark}\label{rem:residual_ODE}
    Another interesting implication is that if we take a look at discrete and continuous kernel dynamics, for very strong residual (when $r\to 0$), the kernel ODE becomes exact. Let us denote the Hermite coefficients of the residual kernel by $\tilde{c}_k.$ For the first term we have $\tilde{c}_1^2 = r^2 c_1^2 + (1-r^2) 1$ and for all other terms $\tilde{c}_k^2 = r^2 c_k.$ Note that as $r \to 0,$ we have $\tilde{c}_1^2\to 1,$ and for all other terms $\tilde{c}_k^2\to 0.$ Thus, in the discrete kernel dynamics, the right hand side will converge to zero. This implies that in the limit of very strong residuals, the kernel ODE gives the exact solution to the kernel dynamics. 
\end{remark}

\begin{remark}\label{rem:no_negative_geometric}
    Suppose there is an activation $\phi$ with negative fixed point $\rho^*<0.$ Let $n$ be an integer such that $\rho^* < -1/(n-1).$ Let $x_1,\dots, x_n\in \R^d$ be vectors that have non zero inner products. Construct an MLP with activation $\phi$ and let $y_1,\dots, y_n$ be the outputs of this MLP. Thus, if the depth is sufficiently large, the pairwise similarity between each pair will converge to $\rho^*.$ Thus, their output Gram matrix $G = [\langle y_i,y_j\rangle]_{i,j\le n}$  has unit diagonals and off diagonals equal to $\rho^*.$ By analyzing the eigenvalues of $G$, we find: The top eigenvalue corresponding to the all-ones eigen vector is: $\lambda_1 = 1 + (n-1)\rho^*$. Because we assumed $\rho^* < -1/(n-1),$ we have $1 + (n-1)\rho^* < 0,$ which is a contradiction. Because $G$ by construction must be positive semi-definite.  
\end{remark}

\begin{theorem}\label{thm:global_attract_centered}
Let $\phi$ be a nonlinear activation with kernel map $\kappa$ that is centered $\kappa(0)=0,$ and $\kappa(1)=1$. 
% \thomas{So you are saying $\{0,1\}$ are fixed points, I guess?} 
Let $\rho_\ell$ be the kernel sequence generated by $\kappa$, given the initial value $\rho_0 \in(-1 , 1)$.
% \thomas{I think you need to exclude the non-stable fixed point. Added the inequality.} 
Then, the sequence contracts towards the fixed point at zero $\rho^*=0$ with rate $\alpha$:
    \begin{align}
        &\frac{|\rho_\ell|}{1-|\rho_\ell|} \le \frac{|\rho_0|}{1-|\rho_0|}\alpha^{\ell}, && \alpha := \frac{1}{2-\kappa'(0)},
    \end{align}
 where $\alpha<1.$ The only other fixed points can be $\rho^*=\pm 1$, and none of them is locally or globally attractive.
\end{theorem}

\begin{remark}
    It is straightforward to show that due to the nonlinear assumption we have $\kappa'(1) = c_1^2 = c_1^2/(\sum_{k=1}^\infty c_k^2) < 1,$ implying $\alpha < 1.$ Thus, the more nonlinear the activation is, the faster the convergence will be. 
\end{remark}

\begin{proof}[Proof of Proposition~\ref{prop:mean_field_kernel_general}]
\thomas{This currently reads a bit hand-wavy. Does the proof eventually move to the appendix? We would have a bit more time to finish this up!?}
 The proof is a straightforward application of the law of large numbers, as the mean field regime implies that the sample means converge to the population means. Let us inductively assume that at layer $\ell$ the it holds that $\frac1d\|h^{\ell}(x)\|^2 = 1$ and $\frac1d\|h^{\ell}(y)\|^2=1,$ and $\frac1d\langle h^{\ell}(x),h^{\ell}(y)\rangle = \rho_{\ell}$. Thus, if $X$ and $Y$ denote the preactivations for a given unit for the two inputs at layer $\ell$, they follow standard Gaussian distribution and have covariance $\rho_{\ell}.$ By construction we have $\E \phi(X)^2 = \E \phi(Y)^2 = 1$, and $\E \phi(X)\phi(Y) = \rho_{\ell+1}.$ Finally, since each hidden unit is independent of others, based on the law of large numbers, we can conclude the samples means will converge to their expectation $\frac1d \|h^{\ell+1}(x)\|^2 = 1, $ $\frac1d \|h^{\ell+1}(y)\|^2 = 1,$ and $\frac1d \langle h^{\ell+1}(x),h^{\ell+1}(y)\rangle = \rho_{\ell+1}. $ This concludes the proof. 
\end{proof}

\begin{proof}[Proof of Mehler's lemma~\ref{lem:mehler_kernel}]
The property can be deduced from Mehler's kernel (cf.~\citep{mehler1866ueber}), which states that 
\begin{align*}
&\frac{1}{\sqrt{1-\rho^2}}\exp\left(-\frac{\rho^2(x^2+y^2)-2xy\rho}{2(1-\rho^2)}\right) \\
&= \sum_{m=0}^\infty \he_m(x)\he_m(y),
\end{align*}
where the $m!$ factor difference is due to the definition of Hermite polynomials with an additional $1/\sqrt{m!}$ compared to the one used in Mehler's kernel. 
Observe that the left-hand side is equal to $p(x,y)/p(x)p(y)$, where $p(x,y)$ is the joint PDF of $(X, Y)$, and $p(x),p(y)$ are marginal PDF of $X$ and $Y$ respectively. Therefore, we can take the expectation using the expansion 
\begin{align*}
\E_{X,Y}&\left[ \he_k(X) \he_l(Y)\right] \\
&= \int \he_k(x)\he_l(y)p(x,y)dx dy \\
&=  \sum_{m=0}^\infty \rho^m\int \he_k(x)\he_l(y) \he_m(x)\he_m(y) dp(x)dp(y)\\
&= \sum_{m=0}^\infty \rho^m\E_{X} \left[\he_k(X)\he_m(X)\right]\E_{Y} \left[\he_l(Y)\he_m(Y)\right] \\
&= \rho^k \delta_{kl} 
\end{align*}
where in the last line we used the orthonormality property $\E[\he_k(X) \he_l(X)]=\delta_{kl}$. 
\end{proof}
% \thomas{I have a suggestion for notation: why don't we use inner product notation for covariance, i.e.$\langle X,Y \rangle = \E[XY]$. This may simplify the notation and avoid clutter. Orthonormality of Hermite polynomials just reads $\langle \he_k, \he_l \rangle = \delta_{kl}$}. \thomas{Another notational shorthand would be to define the bivariate $\mathcal N(\rho)$ as it is needed so often.}




\begin{proof}[Proof of Theorem~\ref{thm:global_attract_centered}]
 The main technical part of the proof is to prove that:
    \begin{align*}
        \Psi(\kappa(\rho)) \le \alpha \Psi(\rho), \quad \text{where} \quad \Psi(\rho) = \frac{|\rho|}{1-|\rho|}.
    \end{align*}
Based on the given assumptions we have $c_0^2 = \kappa(0) = 0$ and based on the assumption $\kappa(1)=1$ we have $\kappa(1)=\sum_{k=1}^\infty c_k^2 = 1.$  
% \thomas{It's confusing as you had made $\kappa(1)=1$ an assumption. I guess it rather follows from energy normalization? Please clarify. If it follows from the $\E\phi^2=1$ condition, then we should use that condition in the theorem.}
    
 First, we will consider positive $\rho\in[0,1),$ for which we have
    \begin{align*}
 \frac{\Psi(\kappa(\rho))}{\Psi(\rho)}&=\left(\frac{\kappa(\rho)}{1-\kappa(\rho)}\right)  \left(\frac{\rho}{1-\rho}\right)^{-1}\\  
        &= \frac{\rho^{-1}\sum_{k=1}^\infty c_k^2 \rho^{k} }{(1-\rho)^{-1}\sum_{k=1}^\infty c_k^2 (1-\rho^{k})} \\
        &= \frac{\sum_{k=1}^\infty c_k^2 \rho^{k-1}}{\sum_{k=1}^\infty c_k^2 \left(\frac{1-\rho^{k}}{1-\rho}\right)}\\
        &= \frac{\sum_{k=1}^\infty c_k^2 \rho^{k-1}}{\sum_{k=1}^\infty c_k^2 \left(\sum_{i=0}^{k-1}\rho^{i}\right)}\\
        &\le \frac{\sum_{k=1}^\infty c_k^2\rho^{k-1} }{\sum_{k=1}^\infty c_k^2 \rho^{k-1} + \sum_{k=2}^\infty c_k^2}\\
        &\le \max_{\rho\in[0,1]} \frac{\sum_{k=1}^\infty c_k^2\rho^{k-1} }{\sum_{k=1}^\infty c_k^2 \rho^{k-1} + \sum_{k=2}^\infty c_k^2}\\
        &= \frac{\sum_{k=1}^\infty c_k^2}{2\sum_{k=1}^\infty c_k^2 -c_1^2 } \\
        &=\frac{1}{2-\kappa'(0)} =: \alpha.
    \end{align*}
% \thomas{I am unsure about the last equality. I see that $\kappa'(0)=c_1^2$. But the infinite sum does not start at $k=0$. So $\sum_{k=1}^\infty c_k^2 = \sum_{k=0}^\infty c_k^2-c_0^2 = 1 - c_0^2$. I don't want to get hung up on this, but maybe you can check.}

% \amir{The reason is that from the assumption we get $\kappa(0) = c_0^2  = 0. $Is it not clear enough? }

% \thomas{Yes, OK it follows from $\phi$ being centered.} 

 Now, we can use the fact that the norm of the sum of values is bounded by the sum of their norms to argue that 
 % \thomas{Shouldn't $k$ start at $k=0$?}
\begin{align*}
 |\kappa(\rho)|=|\sum_{k=1}^\infty c_k^2 \rho^k | &\le \sum_{k=1}^\infty c_k^2 |\rho|^k 
 = \kappa(|\rho|)
\end{align*}
\thomas{I have a naive question. If $\rho > 0$ then this inequality is exact as all terms are positive. It seems that a possible complication only comes from $\rho<0$, where the sign of $\rho^k$ may alternate. Is that correct? Would it make sense to separate out the analysis of both cases?}

\amir{This isn't a naive question at all. It is in a sense more principled to consider positive and negative ranges separately, this is what I did for the ODE version of the problem. In fact, I think using this approach the bound could be improved. 
The main reason the current proof is "magical" is that I didn't have the right intuitions back then and I relied on much more formal steps to prove it. }
 Because $x\mapsto x/(1-x)$ is monotonically increasing for $x\in[0,1]$, for all $\rho\in[-1,1]$ we have 
    \begin{align*}
 \frac{|\kappa(\rho)|}{1-|\kappa(\rho)|}\le \frac{\kappa(|\rho|)}{1-\kappa(|\rho|)} \le \frac{|\rho|}{1-|\rho|}\alpha.
    \end{align*}
Where we invoked the inequality that was proven for $\rho\in[0,1], $ Plugging in the definition of $\Psi$ we have proven that for all $\rho$ we have
\begin{align*}
    \Psi(\kappa(\rho)) \le \alpha \Psi(\rho).
\end{align*}
We can conclude the proof by induction over $\ell.$
\thomas{Except for my question about the constant above, the argument seems valid. I would still like to ponder about the proof a bit more after the first submission deadline.}

\textit{Other fixed points:}
What remains to show is that the only possible fixed points are $\rho^* \in \{0,1,-1\},$ and only $1,-1$ are neither locally or globally attractive.  

First, by contraction result we have so far, for any $\rho \in (-1,1),\rho\neq 0,$ then $|\kappa(\rho)|$ will be strictly smaller than $|\rho|,$ which contradicts with it being a fixed point. That only leaves $\{-1,0,1\}$ as possible fixed points. Now, we will prove that $-1,1$ are not locally attractive fixed points. 

For $\rho^*=1$ to be locally attracting, in some small $\epsilon$ neighborhood of it $\rho\in(1-\epsilon,1),$ $|\kappa(\rho)-1|$ must smaller than $|\rho-1|,$ which contradicts with our contraction result for any $\rho \in (-1,1)$ towards zero. A similar argument shows $-1$ cannot be locally attracting. 

Thus, we have shown that only $\{-1,0,1\}$ can be fixed point, and only $\rho^*=0$ is globally attracting, while $\{-1,1\}$ are neither locally or globally attracting. 
\end{proof}
\thomas{This result seems somewhat discouraging in the sense that there are no interesting (attractive) fixed points (other than orthogonalization). I guess the notion of centered activation functions plays a key role here. Maybe it's worthwhile to provide some orientation about the importance of the $\E\phi=0$ condition earlier in the paper.}


\begin{proof}[Proof of Theorem~\ref{thm:global_attract}]
We will proof all cases individually, starting with the first case that falls directly under a previous theorem. 

\subsection*{Case 1: $\kappa(0)=0$}
We can observe that the case where $\kappa(0)=0,$ falls directly under Theorem~\ref{thm:global_attract}, and thus there is no need to prove it again.

\subsection*{Cases 2,3: $\kappa(0)>0$ and $\kappa'(1)\le 1$} In this part, we jointly consider two cases where $\kappa(0)>0$ and $\kappa'(1) < 1,$ and $\kappa'(1)=1.$ Let us consider the ratio between distances $|\rho_\ell-1|$:
\begin{align*}
    \frac{|\kappa(\rho)-1|}{|\rho-1|} &= \frac{1-\kappa(\rho)}{1-\rho} \\
    &=\frac{\kappa(1)-\kappa(\rho)}{1-\rho}\\
    &=\frac{\sum_{k=1}^\infty c_k^2 (1-\rho^k)}{1-\rho}\\
    &=\sum_{k=1}^\infty c_k^2 \sum_{i=0}^{k-1}\rho^i\\
\implies \frac{|\kappa(\rho)-1|}{|\rho-1|} &=\kappa'(1)-\kappa'(1)+\sum_{k=1}^\infty c_k^2 \sum_{i=0}^{k-1} \rho^i\\
&= \kappa'(1)-\sum_{k=1}^\infty c_k^2 \big(k-\sum_{i=0}^{k-1} \rho^i\big)\\
\end{align*}
Clearly, the term $k-\sum_{i=0}^{k-1} \rho^i$ is is always non-negative. Thus, if $\kappa'(1)<1,$ then we have the contraction 
\begin{align*}
    \frac{|\kappa(\rho)-1|}{|\rho-1|} \le \kappa'(1) \implies |\rho_\ell-1| \le |\rho_0-1| \kappa'(1)^\ell.
\end{align*}
Otherwise, if $\kappa'(1)=1,$ we have 
\begin{align*}
    \frac{|\kappa(\rho)-1|}{|\rho-1|} = 1-\sum_{k=1}^\infty c_k^2 \big(k-\sum_{i=0}^{k-1} \rho^i\big).
\end{align*}
Now, observe that the first term for $k=1$ is zero. Furthermore, the sequence $k-\sum_{i=0}^{k-1}\rho^i$ is monotonically increasing in $k$. Thus, the smallest value the weighted sum can achieve is if all of the weights of terms above $k\ge 2$ are concentrated in $k=2,$ which leads to the contraction
\begin{align*}
    \frac{|\kappa(\rho)-1|}{|\rho-1|} \le 1-(1-c_0^2-c_1^2)(2-1-\rho) \\
    = 1- (1-c_0^2-c_1^2) (1-\rho).
\end{align*}
Now, define sequence $x_\ell:= 1-\rho_\ell,$ and observe that we have 
\begin{align*}
    x_{\ell+1} \le x_\ell (1-\alpha x_\ell), && \alpha = 1-c_0^2-c_1^2,
\end{align*}
where $\alpha > 0$ if the activation is nonlinear. 
We can prove inductively that 
\begin{align*}
x_\ell\le \frac{x_0}{\ell\alpha x_0 + 1}.
\end{align*}
If we plug in the definition of $x_n$ we have proven
\begin{align*}
|\rho_\ell-1| \le \frac{|\rho_0-1|}{\ell \alpha |\rho_0-1| + 1}.
\end{align*}

\subsection*{Case 4: $\kappa(0)>0$ and $\kappa'(1)>1$}
The main strategy is to prove some contraction of $\kappa(\rho)$ towards $\rho^*$, under the kernel map $\kappa$. In other words, we need to show $|\kappa(\rho)-\rho^*|$ is smaller than $|\rho-\rho^*|$ under some potential. First, we assume there is a $\rho^*$ such that $\kappa'(\rho^*)<1,$ and show this contraction, and later prove its existence and uniqeness. 

To prove contraction towards $\rho^*$ when $\kappa'(\rho^*)<1$, we consider three cases: 1) If $\rho > \rho^*$, 2) If $\rho \in [0,\rho^*]$, and 3) If $\rho < 0$. However, the bounds will be of different potential forms and will have to be combined later.  Let $\kappa(\rho) = \sum_{k=0}^\infty c_k^2 \rho^k$ be the kernel map with $\kappa(1)= 1$ with fixed point $\rho^*$ that satisfies $\kappa'(\rho^*)<1.$

\begin{itemize}
\item \textbf{$\rho\ge \rho^*$.} we will prove:
\begin{align*}
\frac{|\kappa(\rho)-\rho^*|}{1-\kappa(\rho)} \le \frac{|\rho-\rho^*|}{1-\rho} \kappa'(\rho^*)
\end{align*}

We have the series expansion around $\rho^*$: $\kappa(\rho) = \rho^* + \sum_{k=1}^\infty a_k (\rho-\rho^*)^k$. For points $\rho\ge \rho^*$, we will have $\kappa(\rho)\ge \rho^*$, thus we can write

\begin{align*}
&\frac{\kappa(\rho)-\rho^*}{1-\kappa(\rho)} \\
&= \frac{\sum_{k=1}^\infty a_k (\rho-\rho^*)^k}{\kappa(1)- \kappa(\rho^*)}\\
&= \frac{\sum_{k=1}^\infty a_k (\rho-\rho^*)^k}{\sum_{k=1}^\infty a_k (1-\rho^*)^k - \sum_{k=1}^\infty a_k (\rho-\rho^*)^k}  \\
% &= \frac{(\rho-\rho^*)\sum_{k=1}^\infty a_k (\rho-\rho^*)^{k-1}}{(1-\rho)\sum_{k=1}^\infty a_k (\sum_{i=0}^{k-1}(1-\rho^*)^i(\rho-\rho^*)^{k-1-i})} \\
&= \frac{\rho-\rho^*}{1-\rho}\cdot\frac{\sum_{k=1}^\infty a_k (\rho-\rho^*)^{k-1}}{\sum_{k=1}^\infty a_k (\sum_{i=0}^{k-1}(1-\rho^*)^i(\rho-\rho^*)^{k-1-i})}\\
&\le \frac{\rho-\rho^*}{1-\rho}\frac{\sum_{k=1}^\infty a_k (\rho-\rho^*)^{k-1}}{\sum_{k=2}^\infty a_k (1-\rho^*)^{k-1} + \sum_{k=1}^\infty a_k (\rho-\rho^*)^{k-1}},
\end{align*}
where in the last line the inequality is due to the fact that we are only retaining the terms corresponding to $i=0$ and $i=k-1$ in the denominator.
Now, note the right hand side maximizes when $\rho\in[\rho^*,1]$ is maximized, which is obtained when $\rho=1$:
\begin{align*}
&\frac{\kappa(\rho)-\rho^*}{1-\kappa(\rho)}\\
&\le \frac{\rho-\rho^*}{1-\rho}\frac{\sum_{k=1}^\infty a_k (1-\rho^*)^{k-1}}{\sum_{k=2}^\infty a_k (1-\rho^*)^{k-1} + \sum_{k=1}^\infty a_k (1-\rho^*)^{k-1}}.
\end{align*}
Now we can observe that the numerator and denominator correspond to 
\begin{align*}
    &=\frac{\rho-\rho^*}{1-\rho}\frac{\kappa(1)-\rho^*}{2\kappa(1)-\kappa'(\rho^*)}\\
&=\frac{\rho-\rho^*}{1-\rho}\frac{1-\rho^*}{2-\kappa'(\rho^*)}
\end{align*}
Thus, we have proven that 
\begin{align*}
\rho \ge \rho^* \implies \frac{|\kappa(\rho)-\rho^*|}{1-\kappa(\rho)} \le \frac{|\rho-\rho^*|}{1-\rho} \frac{1-\rho^*}{2-\kappa'(\rho^*)}
\end{align*}

% Now, because we can view the numerator and denominator as weighted sums, with $a_k$ as weights, and considering that the ratio between $(\rho-\rho^*)^{k-1}$ and $\sum_{i=0}^{k-1}(1-\rho^*)^i(\rho-\rho^*)^{k-1-i}$ is monotonically increasing in $\rho$, we can conclude that the ratio is maximized at $\rho$ is maximum, i.e., $\rho=1$. Thus, we can write
% \begin{align*}
% \rho \ge \rho^* \implies \frac{|\kappa(\rho)-\rho^*|}{1-\kappa(\rho)} \le \frac{|\rho-\rho^*|}{1-\rho} \frac{\sum_{k=1}^\infty a_k (1-\rho^*)^{k-1}}{\sum_{k=1}^\infty k a_k (1-\rho^*)^{k-1}} = \frac{|\rho-\rho^*|}{1-\rho} \frac{1-\rho^*}{\kappa'(1)}
% \end{align*}
% For the last step, we can recognize the quantities numerator quantity as $\kappa(1)-\rho^* = 1-\rho^*,$ while the denominator is $\kappa'(1)$. Thus, we have proven 
% \begin{align*}
% \rho \ge \rho^* \implies \frac{|\kappa(\rho)-\rho^*|}{1-\kappa(\rho)} \le \frac{|\rho-\rho^*|}{1-\rho} \frac{1-\rho^*}{\kappa'(1)}
% \end{align*}

\item \textbf{$0\le \rho \le \rho^*$.}
Consider $\rho \in [0,\rho^*]$. For these $\kappa'(\rho)$ is always monotonically increasing, implying that $\kappa'(\rho)\le \kappa'(\rho^*) < 1$. Thus, $|\kappa(\rho)-\rho^*| \le \kappa'(\rho^*) |\rho - \rho^*|$. Thus, by Banach fixed point theorem, we have that $\kappa$ is a contraction in this range with a rate $\kappa'(\rho^*)$:
\begin{align*}
0\le \rho \le \rho^* \implies |\kappa(\rho) - \rho^*| \le \kappa'(\rho^*) |\rho-\rho^*|
\end{align*}

\item \textbf{$-1 \le \rho \le 0$.}
Finally, let us consider $\rho \le 0$. 
Recall that we have $\kappa(1) = 1$. Thus, we can express $\kappa(\rho)-1$ as product of $(\rho-1)$ with some power series $q(\rho)$:
\begin{align*}
\kappa(\rho) - 1 = (\rho-1)q(\rho), \quad q(\rho) = \sum_{k=0}^\infty b_k \rho^k.
\end{align*}
In fact, we can expand $\kappa(\rho)$ in terms of these new coefficients
\begin{align*}
\kappa(\rho) = 1-b_0 + \sum_{k=0}^\infty (b_k - b_{k+1}) \rho^k
\end{align*}
Due to the non-negativity of coefficients of $\kappa$, we can conclude $1\ge b_0 \ge b_1 \ge ...$. Based on this observation, for $0 < \rho < 1$, we can conclude
$q(-\rho) = b_0 - b_1 \rho + b_2 \rho^2 - ... \le b_0$. Because we can pair each odd and even term $-b_{k}\rho^k + b_{k+1} \rho^{k+1}$ for all odd $k$, and because coefficients $b_k \ge b_{k+1}$ and $\rho^{k} \ge \rho^{k+1}$ for $\rho \in [0,1]$, we can argue $q(-\rho) \le b_0 = 1 - c_0^2$. Now, plugging this value into the kernel map for $0 < \rho < 1$, we have:
\begin{align*}
\kappa(-\rho) &= 1 - (1+\rho)q(-\rho) \\
&\ge 1-(1+\rho)(1-c_0^2)\\
&= 1 - 1 - \rho + c_0^2 (1+\rho)\\
&\implies \kappa(-\rho) + \rho \ge c_0^2 (1+\rho)\\
&\implies -\rho + c_0^2(1+\rho) \le \kappa(-\rho) \le \kappa(\rho)
\end{align*}
Now, if we assume $\kappa(-\rho) \le \rho^*$ then
\begin{align*}
\frac{|\kappa(-\rho)-\rho^*|}{|-\rho-\rho^*|}&=\frac{\rho^* - \kappa(-\rho)}{\rho^* + \rho} \\
&\le \frac{\rho^* + \rho - c_0^2 (1+\rho)}{\rho+\rho^*} \\
&= 1- \frac{c_0^2(1+\rho)}{\rho+\rho^*} \\
&\le 1-c_0^2\\
&= 1 - \kappa(0).
\end{align*}
Now, if we assume $\kappa(-\rho) \ge \rho^*$, knowing that $\kappa(-\rho)\le \kappa(\rho)$, which necessitates $\rho\ge \rho^*$, which implies $\kappa(\rho)\le \rho$. Thus, we have
\begin{align*}
\frac{|\kappa(-\rho)-\rho^*|}{|-\rho-\rho^*|} &= \frac{\kappa(-\rho)-\rho^*}{\rho+\rho^*}\\
&\le \frac{\kappa(-\rho)-\rho^*}{\rho+\rho^*} \\
&\le \frac{\rho-\rho^*}{\rho+\rho^*} \\
&\le \frac{1-\rho^*}{1+\rho^*} \\
&\le 1-\rho^*
\end{align*} 
Combining both cases we have 
\begin{align*}
\rho \le 0 \implies \frac{|\kappa(\rho)-\rho^*|}{|\rho-\rho^*|} &\le 1-\min(\kappa(0),\rho^*).
\end{align*}
We can further prove that $\rho^* = \kappa(\rho^*) = k(0) + \text{non-negative terms}$, which implies that $\rho^* \ge k(0)$. Thus, we can conclude that 
\begin{align*}
\rho \le 0 \implies |\kappa(\rho)-\rho^*| \le (1 - k(0))|\rho-\rho^*|
\end{align*}
\end{itemize}



\textbf{Combining the cases}
Let us summarize the results so far. We have proven the existence of a unique fixed point $\rho^*\in[0,1]$ such that $\kappa'(\rho^*)< 1,$ and we have proven contraction rates for each of the three cases.
\begin{align*}
\begin{cases}
\frac{|\kappa(\rho)-\rho^*|}{1-\kappa(\rho)} \le \frac{|\rho-\rho^*|}{1-\rho}\frac{1-\rho^*}{2-\kappa'(\rho^*)} & \text{if } \rho \ge \rho^* \\
|\kappa(\rho)-\rho^*| \le \kappa'(\rho^*)|\rho-\rho^*| & \text{if } 0 \le \rho \le \rho^*\\
|\kappa(\rho)-\rho^*| \le (1 - k(0))|\rho-\rho^*| & \text{if } \rho \le 0 
\end{cases}
\end{align*}

% Again, we can consider two cases, if $\kappa'(1)<1$ or $\kappa'(1)>1.$ First, note that $\kappa'(1)=1$ is impossible, as it would imply $\kappa'(1)=\sum_{k=1}^\infty k c_k^2 = \sum_{k=0}c_0^2 ,$ which implies that all $c_k=0,$ which is a contradiction with the assumption that the activation is nonlinear.

Let us now define the joint decay rate:
\begin{align*}
\alpha = \max\left\{1 - k(0), \kappa'(\rho^*), \frac{1-\rho^*}{1-\kappa'(\rho^*)}\right\}
\end{align*}
In other words, this is the worst-case rate for any of the above cases. 

Now, let us assume we are starting from initial $\rho_0$ and define $\rho_\ell = \kappa(\rho_{\ell-1})$. One important observation is that if we have $\rho_0 \ge \rho^*$ then by monotonicity of $\kappa$ in the $[0,1]$ range, it will remain the same range, and similarly if $\rho_0\in[0,\rho^*]$ it will remain in the same range. Thus, from that index onwards, we can apply the contraction rate of the respective case. The only case that there might be a transition is if $\rho_0 < 0$. 

Assuming that $\rho_0 < 0$, let $\rho_\ell$ be the first index that we have $\rho_\ell \ge 0$. Thus, from $\rho_0$ to $\rho_\ell$ we can apply the contraction rate of the third case:
\begin{align*}
|\rho_\ell - \rho^*| \le |\rho_0 - \rho^*| \alpha^\ell
\end{align*}

Now, we have two possibilities, either $\rho_\ell \ge \rho^*$ or $\rho_\ell \le \rho^*$. If $\rho_\ell \ge \rho^*$, we can apply the contraction rate of the first case, and if $\rho_\ell \le \rho^*$ we can apply the contraction rate of the second case:
\begin{align*}
    \begin{cases}
|\rho_L - \rho^*| \le |\rho_\ell - \rho^*| \alpha^{L-\ell} & 0\le \rho_\ell\le \rho^*\\
\frac{|\rho_L-\rho^*|}{1-\rho_L} \le \frac{|\rho_\ell-\rho^*|}{1-\rho_\ell}\alpha^{L-\ell} & \rho_\ell\ge \rho^* \\
    \end{cases}
\end{align*}
If we plug in our contraction up to step $\ell$ and use the fact that the norm of the sequence is non-increasing $|\rho_0|\ge \rho_\ell,$ we have
\begin{align*}
    \begin{cases}
|\rho_L - \rho^*| \le |\rho_0 - \rho^*| \alpha^{L} & 0\le \rho_\ell\le \rho^*\\
\frac{|\rho_L-\rho^*|}{1-\rho_L} \le \frac{|\rho_0-\rho^*|}{1-|\rho_0|}\alpha^{L} & \rho_\ell\ge \rho^* \\
    \end{cases}
\end{align*}
We can now take the worst case of these two and conclude that
\begin{align*}
|\rho_L - \rho^*| \le \frac{|\rho_0 - \rho^*|}{1-|\rho_0|} \alpha^L
\end{align*}

So far in the proof, we assumed the existence of $\rho^*$ that obeys $\kappa'(\rho^*)<1.$ We can now prove that such a fixed point exists. It is unique, and it is necessarily in the range $\rho^*\in [0,1].$

\textbf{Positivity, uniqueness, and existence of a globally attractive fixed point}
Here, the goal is to prove there is exactly one point $\rho^*\in[0,1]$ such that $\kappa(\rho^*) = \rho^*$ and $\kappa'(\rho^*) < 1.$ We will prove the properties of positivity, uniqueness, and existence separately.

\textit{Positivity:} Let us assume that $\rho^* \le 0.$ is a fixed point. Then, we can apply the contraction rate proven for Case 3, which shows that $\kappa(\rho^*)\ge \rho^* + k(0) > \rho^*,$ which is a contradiction.


\textit{Uniqueness:} Assume that there are two fixed points $\rho_1$ and $\rho_2$ that satisfy $\kappa'(\rho_1),\kappa'(\rho_2)<1.$ Let us assume wlog that $\rho_1 < \rho_2.$ Then we can invoke the contraction rate proven so far to argue that all points in $(-1,1),$ including  $\rho\in (\rho_1,\rho_2)$ are attracted towards both $\rho_1$ and $\rho_2,$ which is a contradiction. Thus, there can be at most one fixed point.

\textit{Existence of $\rho^*$:} Because $\kappa(1)=1$ the set of all fixed points is non-empty. Let us assume that $\rho^*$ is the first (smallest) fixed point of $\kappa(\rho^*) = \rho^*,$ which because of the positivity result is necessarily $\rho^*>0.$ If we assume that $\kappa'(\rho^*) > 1,$ then in the small $\epsilon$-neighborhood of it $\rho_1 \in(\rho^*-\epsilon,\rho^*)$ we have $\kappa(\rho_1) < \rho_1.$ Because $\kappa(\rho)$ is continuous, and is above identity line at $\rho=0$ and under identity line $\rho=\rho_1,$ there must be a point $0 < \rho_2 < \rho_1$ where it is at identity $\kappa(\rho_2) = \rho_2,$ which is a contradiction with assumption that $\rho^*$ is the smallest fixed point. Thus, we must have $\kappa'(\rho^*) \le 1.$ If we assume that $\kappa'(\rho^*) = 1,$ then the $\kappa$ must align with the identity line from $\rho^*$ to $1,$ which implies that all higher order terms $c_k,k\ge 2$ must be zero, which in turn implies that $\kappa$ is a linear function. This is a contradiction with the assumption that the activation is nonlinear. Thus, we must have $\kappa'(\rho^*) < 1,$ which proves the desired existence. 
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:residual_kernel_map}]
First, we ought to prove that in the mean field regime, the kernel map is transformed according to the equation that is given. To do so, we can consider one layer update. Assume that $X,Y\sim N(0,1)$ with covariance $\E XY = \rho.$ Now, $(X',Y')$ is an independent copy of $(X,Y),$ with the same variance and covariance structure. This is due to the presence of skip connection weights $P.$ Now, defined the joint layer update 
\begin{align*}
\psi(X) &= \sqrt{1-r^2} \phi(X)  + r X',\\
\psi(Y) &= \sqrt{1-r^2} \phi(Y')  + r Y',\\
\implies \kappa_\psi(\rho)&:= \E \psi(X)\psi(Y) \\
&=  (1-r^2) \E \phi(X)\phi(Y) + r^2 \E X' Y' \\
    &= (1-r^2) \kappa(\rho) + r^2 \rho,
\end{align*}
where in the third line we use the independence assumption and the fact that $X'$ and $Y'$ have zero mean. 

Observe that $\kappa_\psi(\rho) = (1-r^2) \kappa(\rho),$ or $\kappa_\psi'(\rho) = (1-r^2) \kappa'(\rho) + r^2.$ Now, let us consider the conditions for the four cases of Theorem~\ref{thm:global_attract}. The decision boundaries are if $\kappa(0)$ is positive or not, if $\kappa'(1)$ is above, equal to, or below $1.$ Now, note that for $r \in (0,1),$ strict positivity of $\kappa_\psi(0) = (1-r^2) \kappa(0)$ remains the same as $\kappa(0).$ Furthermore, $\kappa_\psi'(1)$ is a weighted average of $\kappa'(1)$ and $1.$ Thus, for all values $r\in(0,1),$ it holds $\kappa_\psi'(1)$ is above, equal to, or below $1,$ if and only if $\kappa'(1)$ has the corresponding property.  Now, we can turn our focus on the convergence rates. For this, we can focus on $\alpha$ in each one of the four cases. 
\begin{itemize}
    \item If $\kappa(0)=0$ then $\kappa_\psi(0) = r^2 \kappa(0)$ and we have $\alpha = 1 / (2-\kappa_\psi'(0)).$ Now, note that $\kappa_\psi'(0) = (1-r^2)\kappa'(0) +  r^2$ is a weighted average between $\kappa'(0) < 1$ and $1,$ and thus, the larger the value of $r,$ the larger $\alpha$ would be, and the slower the convergence. 
    \item If $\kappa(0)>0$ and $\kappa'(1)<1$ then we have $\kappa_\psi(0)=0$ and $\kappa_\psi'(1) < 1,$ and $\alpha = \kappa_\psi'(1) = (1-r^2) \kappa'(1) + r^2.$ Thus, the larger the residual, the closer $\alpha$ becomes to $1,$ and the slower the convergence will be.  
    \item If $\kappa(0)>0$ and $\kappa'(1)=1,$ then we have the same for $\kappa_\psi,$ and 
    \begin{align*}
        \alpha &= 1 - \kappa_\psi(0) - \kappa_\psi'(0)\\
        &= 1 - (1-r^2) \kappa(0) - (1-r^2)\kappa'(0) - r^2\\
        &= (1-r^2 )( 1 - \kappa(0) - \kappa'(0))
    \end{align*}
    Now, recall that we have $\kappa(0) = c_0^2$ and $\kappa'(0)=c_1^2,$ and thus we have $1-\kappa(0) - \kappa'(0) = \sum_{k=2}^\infty,$ which is necessarily positive for a nonlinear activation:
    \begin{align*}
        \alpha = (1-r^2) \sum_{k=2}^\infty c_k^2.
    \end{align*}
    We can now see that for larger $r,$ $\alpha $ will be smaller, which in this case implies a slower convergence. 
    \item If $\kappa(0)>0$ and $\kappa'(1) > 1$ then same holds for $\kappa_\psi,$ and the convergence rate $\alpha$ .
\end{itemize}

\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:normalization_kernel_map}]
The first part, i.e., the no normalization case, follows directly from Theorem~\ref{thm:global_attract}. The remainder of the proof follows from the following observation:

Let us assume that $\phi$ is square-integrable $\E \phi(X)^2$. Then, the following hold in the mean field regime and $X\sim N(0,1).$ We have 
\begin{itemize}
    \item For MLP with post-RMS, pre-RMS, and pre-LN, the normalization layer will converge to  $z\to z/\sqrt{\E \phi(X)^2}$.
    \item In MLP with post-LN, the normalization layer will converge to $z \to  (z-\mu)/\phi$, where the $\mu = \E[\phi(X)]$ and $\phi = \sqrt{\E (\phi(X) - m)^2 }.$ 
\end{itemize}

Let us consider each case separately.

\begin{itemize}
    \item \textbf{Pre-act RMS and pre-act LN:} 
 Let us define $s:= \sqrt{\E \phi(X)^2}.$ Let us inductively assume that $\frac1d \|h^{\ell-1}\| = s.$ Thus, elements of $a:= W^\ell h^{\ell-1}$ are drawn i.i.d/ from $N(0,s^2).$ Thus, in the mean field regime, the centering step of LN will be ineffective, and the normalization step of both LN and RMS will converge to $s.$ Thus, normalization will result in a Gaussian vector with its elements drawn i.i.d. from $N(0,1).$ Thus, the norm of the activations will be, by definition, for each element, we have $\E \phi(a_i)^2 = s^2.$ Finally, in the mean field regime, the sample mean will converge to population mean $\frac1d\|h^\ell\|^2 = \frac1d\|\phi(a)\|^2 = s^2 $, proving the induction hypothesis. In the process, we have also proved that in the mean field, both pre-act RMS and pre-act LN will converge to $z\to z/ s.$

\item \textbf{Post-act RMS:} Again, let us define $s:= \sqrt{\E \phi(X)^2}.$ Because $h^{\ell-1}$ is defined after the normalization. we have $\frac1d \|h^{\ell-1}\| = 1.$ Thus, elements of $a:= W^\ell h^{\ell-1}$ are drawn i.i.d from $N(0,1).$ Thus, after going through activation $\phi(a^\ell),$ for each element have $\E \phi(a_i)^2 = s^2.$ and therefore in the mean field, the sample mean will converge to population mean $\frac1d\|h^\ell\|^2 = s^2 $, which proves the claim. Thus, RMS normalization will converge to $z \to z/ s.$
    \item \textbf{Post-act LN:} The analysis is similar to above, except for the last step, layer normalization will subtract sample mean and sample standard deviation. In the mean field, both of these quantities converge to their population counterparts $\mu = \E \phi(X),$ and $\sigma = \sqrt{\E (\phi(X) - \mu)^2 },$ proving that the post-act LN will act like $z \to  (z-\mu)/\sigma.$ 
\end{itemize}

Now that we have established that in all these cases, we can replace the normalization and activation layer with a new activation $\ Psi,$ that obeys $\E \psi(X)^2 = 1,$ up to some absolute constant scale. The only difference is that in post-act LN, this new activation will also be centered $\E \psi(X) = 0,$ while in the other three cases, if and only if $\phi$ is centered:

\begin{itemize}
    \item For pre-act RMS and pre-act LN, and post-act RMS, we can construct a new activation $\psi$ that obeys $\E \psi(X)^2 = 1,$ and if $\phi$ is centered, the new activation is centered $\E \psi(X) = 0.$ Thus, assuming that the original activation is centered, we can involve Theorem~\ref{thm:global_attract} to conclude the proof.
    \item For post-act LN, we will get a new activation $\psi$ that obeys $\E \psi(X)^2 = 1,$ and $\E \psi(X) = 0.$ Thus we can invoke Theorem~\ref{thm:global_attract} to conclude the proof.
\end{itemize}
\end{proof}

\begin{corollary}\label{cor:double_exp}
    If Hermite coefficients of $\phi$ has Hermite expansion $\phi = \sum_{k=m}^\infty c_k \he_k,$ then kernel sequence of MLP with activation $\phi$ converges to zero with double exponential rate 
    \begin{align*}
        |\rho_\ell| \le |\rho_0|^{m^\ell}.
    \end{align*}
\end{corollary}


\section{Validation of the global convergence theorem}\label{sec:experiments}
Here we will provide some numerical validation of the global convergence theorem. We will consider the kernel map for some custom made and commonly used activations, and show that the fixed point $\rho^*$ is globally attractive. We will consider the kernel map for the following activations: $\phi(x) = \tanh(x), \phi(x) = \max(0,x), \phi(x) = \exp(x), \phi(x) = \text{GELU}(x),$ which will correspond to the four cases of the theorem. See Figure~\ref{fig:validation_plots} and Figure~\ref{fig:validation_plots_real_activations} for the results.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_0.pdf}
        \caption{\small Case 1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_1.pdf}
        \caption{\small Case 2}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_2.pdf}
        \caption{\small Case 3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_3.pdf}
        \caption{\small Case 4}
    \end{subfigure}
    \caption{\small Validation of Theorem~\ref{thm:global_attract}, each corresponding to one of the four cases of the theorem. Left column shows the activation $\phi.$ The middle and right columns show the kernel map show fixed point iteration starting from $\rho_0,$ and applying $\rho_{\ell+1}=\kappa(\rho_\ell)$ for many steps. The middle column shows the kernel map, while the right column shows the distance to the fixed point $\rho^*$.}
    \label{fig:validation_plots}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_0_tanh.pdf}
        \caption{\small Case 1. Tanh activation.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_1_relu.pdf}
        \caption{\small Case 2: ReLU activation.}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_2_exp.pdf}
        \caption{\small Case 3: Exponential activation.} 
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{./kernel_map_convergence_case_3_gelu.pdf}
        \caption{\small Case 4: GELU activation.}
    \end{subfigure}
    \caption{\small Same as Figure~\ref{fig:validation_plots} for some commonly used activations. Note that because the raw activations do not necessarily obey $\E \phi^2(X)=1,$ we have to scale by some constant $C$ to make the activations obey the conditions of the theorem.}
    \label{fig:validation_plots_real_activations}
\end{figure*}

\end{document}
